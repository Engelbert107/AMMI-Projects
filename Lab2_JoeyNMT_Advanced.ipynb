{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Lab2_JoeyNMT_Advanced.ipynb","provenance":[{"file_id":"https://github.com/juliakreutzer/AMMI_NMT/blob/main/Lab2_JoeyNMT_Advanced.ipynb","timestamp":1621346603784}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"KRwdfvuQ486V"},"source":["# Lab 2: Joey NMT Advanced\n","\n","In this notebook, we'll train a Transformer model for translating between TED talks in French (*fr*) and English (*en*). We'll do some configuration debugging and experiment with hyperparameters, then inspect evaluation metrics and find out how robust the model is.\n","\n","The pre-processing code is a bit lengthy, but it reflects reality: often getting the data into the right format and selecting the right pieces takes more code than the actual model training ;) \n","\n","At the very end of this colab you'll also find instructions on how to get started with backtranslation as a data augmentation technique, and how to build a multilingual model. These topics are not mandatory but might be fun to explore if you have time. \n","\n","**Important:** Before you start, set runtime type to GPU.\n","\n","Author: Julia Kreutzer"]},{"cell_type":"code","metadata":{"id":"Qx1neLQt3vZ0","executionInfo":{"status":"ok","timestamp":1621427670294,"user_tz":0,"elapsed":2387,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["import os"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0t24FKds4_oj","executionInfo":{"status":"ok","timestamp":1621427840396,"user_tz":0,"elapsed":163718,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}},"outputId":"cc20e520-1160-4b26-c592-20b620b511f8"},"source":["!pip install torch==1.8.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.8.0+cu101\n","\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.8.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (763.5MB)\n","\u001b[K     |████████████████████████████████| 763.5MB 24kB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (3.7.4.3)\n","\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.8.0+cu101 which is incompatible.\u001b[0m\n","\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.8.0+cu101 which is incompatible.\u001b[0m\n","Installing collected packages: torch\n","  Found existing installation: torch 1.8.1+cu101\n","    Uninstalling torch-1.8.1+cu101:\n","      Successfully uninstalled torch-1.8.1+cu101\n","Successfully installed torch-1.8.0+cu101\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"g_-Qu5xo5ILb","executionInfo":{"status":"ok","timestamp":1621427862088,"user_tz":0,"elapsed":181974,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}},"outputId":"a22e0f2a-f58c-4163-e161-e7f637c73d11"},"source":["!pip install joeynmt"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting joeynmt\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/0a/f383560ca9eedbd4a09f5d9d1523aaf3b4b5e0e79a1e5d9c72ff370826d4/joeynmt-1.3-py3-none-any.whl (84kB)\n","\u001b[K     |████████████████████████████████| 92kB 4.2MB/s \n","\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt) (56.1.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt) (0.16.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt) (7.1.2)\n","Collecting sacrebleu>=1.3.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n","\u001b[K     |████████████████████████████████| 61kB 8.5MB/s \n","\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt) (0.11.1)\n","Collecting pylint\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/f0/9705d6ec002876bc20b6923cbdeeca82569a895fc214211562580e946079/pylint-2.8.2-py3-none-any.whl (357kB)\n","\u001b[K     |████████████████████████████████| 358kB 21.3MB/s \n","\u001b[?25hCollecting torchtext==0.9.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/50/84184d6230686e230c464f0dd4ff32eada2756b4a0b9cefec68b88d1d580/torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1MB)\n","\u001b[K     |████████████████████████████████| 7.1MB 18.0MB/s \n","\u001b[?25hRequirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt) (1.8.0+cu101)\n","Collecting six==1.12\n","  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n","Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt) (2.4.1)\n","Collecting wrapt==1.11.1\n","  Downloading https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt) (3.2.2)\n","Collecting pyyaml>=5.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n","\u001b[K     |████████████████████████████████| 645kB 43.8MB/s \n","\u001b[?25hCollecting subword-nmt\n","  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n","Collecting numpy==1.20.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/8a/064b4077e3d793f877e3b77aa64f56fa49a4d37236a53f78ee28be009a16/numpy-1.20.1-cp37-cp37m-manylinux2010_x86_64.whl (15.3MB)\n","\u001b[K     |████████████████████████████████| 15.3MB 217kB/s \n","\u001b[?25hCollecting portalocker==2.0.0\n","  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n","Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt) (1.4.1)\n","Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt) (1.1.5)\n","Collecting astroid<2.7,>=2.5.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/82/a61df6c2d68f3ae3ad1afa0d2e5ba5cfb7386eb80cffb453def7c5757271/astroid-2.5.6-py3-none-any.whl (219kB)\n","\u001b[K     |████████████████████████████████| 225kB 51.6MB/s \n","\u001b[?25hRequirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt) (0.10.2)\n","Collecting isort<6,>=4.2.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/47/0ec3ec948b7b3a0ba44e62adede4dca8b5985ba6aaee59998bed0916bd17/isort-5.8.0-py3-none-any.whl (103kB)\n","\u001b[K     |████████████████████████████████| 112kB 42.9MB/s \n","\u001b[?25hCollecting mccabe<0.7,>=0.6\n","  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->joeynmt) (3.7.4.3)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (1.30.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (3.3.4)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (0.36.2)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (2.0.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (1.32.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (3.12.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (0.4.4)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (1.8.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt) (0.12.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt) (1.3.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt) (2.4.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt) (0.10.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt) (2018.9)\n","Collecting lazy-object-proxy>=1.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/b0/f055db25fd68ab4859832a887c8b304274fc12dd5a3f8e83e61250733aeb/lazy_object_proxy-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (55kB)\n","\u001b[K     |████████████████████████████████| 61kB 10.0MB/s \n","\u001b[?25hCollecting typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/b3/573d2f1fecbbe8f82a8d08172e938c247f99abe1be3bef3da2efaa3810bf/typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n","\u001b[K     |████████████████████████████████| 747kB 45.1MB/s \n","\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.0->joeynmt) (2.10)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (4.2.2)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (4.7.2)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt) (4.0.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt) (1.3.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt) (0.4.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.15->joeynmt) (3.4.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt) (3.1.0)\n","Building wheels for collected packages: wrapt\n","  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wrapt: filename=wrapt-1.11.1-cp37-cp37m-linux_x86_64.whl size=68377 sha256=38bfc2f8c475fad5e6db45f3b6a7f38feda1bb599aeb3483e1bbd4e26ac75457\n","  Stored in directory: /root/.cache/pip/wheels/89/67/41/63cbf0f6ac0a6156588b9587be4db5565f8c6d8ccef98202fc\n","Successfully built wrapt\n","\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.8.0+cu101 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.20.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.4.1 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.4.1 has requirement wrapt~=1.12.1, but you'll have wrapt 1.11.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-api-python-client 1.12.8 has requirement six<2dev,>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-api-core 1.26.3 has requirement six>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: portalocker, sacrebleu, lazy-object-proxy, typed-ast, wrapt, astroid, isort, mccabe, pylint, numpy, torchtext, six, pyyaml, subword-nmt, joeynmt\n","  Found existing installation: wrapt 1.12.1\n","    Uninstalling wrapt-1.12.1:\n","      Successfully uninstalled wrapt-1.12.1\n","  Found existing installation: numpy 1.19.5\n","    Uninstalling numpy-1.19.5:\n","      Successfully uninstalled numpy-1.19.5\n","  Found existing installation: torchtext 0.9.1\n","    Uninstalling torchtext-0.9.1:\n","      Successfully uninstalled torchtext-0.9.1\n","  Found existing installation: six 1.15.0\n","    Uninstalling six-1.15.0:\n","      Successfully uninstalled six-1.15.0\n","  Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed astroid-2.5.6 isort-5.8.0 joeynmt-1.3 lazy-object-proxy-1.6.0 mccabe-0.6.1 numpy-1.20.1 portalocker-2.0.0 pylint-2.8.2 pyyaml-5.4.1 sacrebleu-1.5.1 six-1.12.0 subword-nmt-0.3.7 torchtext-0.9.0 typed-ast-1.4.3 wrapt-1.11.1\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy","six"]}}},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"sNHhMgH55P8l"},"source":["# Data Preparation\n","\n","We'll use *English - French* translations from the [IWSLT 2017 challenge](https://wit3.fbk.eu/2017-01-c), the [\"unofficial\" task](https://sites.google.com/site/iwsltevaluation2017/TED-tasks?authuser=0). This challenge is about translating TED talks from multiple languages."]},{"cell_type":"markdown","metadata":{"id":"2KrEuHrz5dEt"},"source":["## Download"]},{"cell_type":"markdown","metadata":{"id":"3PTCwLsM5_He"},"source":["Requires downloading a file of 292MB. If you do this ahead of time, store a copy of it in your Google drive and access it from there."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0qUEf7DX5whA","executionInfo":{"status":"ok","timestamp":1621427879462,"user_tz":0,"elapsed":3571,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}},"outputId":"4b0bb385-1d07-4919-86ed-671d0d952fad"},"source":["! pip install gdown"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.12.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.41.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H3B9ejpL5LFx","executionInfo":{"status":"ok","timestamp":1621427886948,"user_tz":0,"elapsed":9291,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}},"outputId":"d7d415e4-c697-48f6-ee57-64d63d990f72"},"source":["!gdown https://drive.google.com/uc?id=1gFeuPTRc3RB4DhJEkhr8O-a8PObM7Ix2"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1gFeuPTRc3RB4DhJEkhr8O-a8PObM7Ix2\n","To: /content/2017-01-trnted.tgz\n","292MB [00:04, 59.2MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sJuXNbBA5ly6","executionInfo":{"status":"ok","timestamp":1621427890077,"user_tz":0,"elapsed":9915,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}},"outputId":"5ed04098-1a0e-4ce4-e937-f0e72bcc407b"},"source":["! tar -zxvf /content/2017-01-trnted.tgz"],"execution_count":6,"outputs":[{"output_type":"stream","text":["2017-01-trnted/\n","2017-01-trnted/texts/\n","2017-01-trnted/._texts.html\n","2017-01-trnted/texts.html\n","2017-01-trnted/texts/ar/\n","2017-01-trnted/texts/de/\n","2017-01-trnted/texts/en/\n","2017-01-trnted/texts/fr/\n","2017-01-trnted/texts/ja/\n","2017-01-trnted/texts/ko/\n","2017-01-trnted/texts/zh/\n","2017-01-trnted/texts/zh/en/\n","2017-01-trnted/texts/zh/en/._.eval\n","2017-01-trnted/texts/zh/en/.eval\n","2017-01-trnted/texts/zh/en/._.info\n","2017-01-trnted/texts/zh/en/.info\n","2017-01-trnted/texts/zh/en/._zh-en.tgz\n","2017-01-trnted/texts/zh/en/zh-en.tgz\n","2017-01-trnted/texts/ko/en/\n","2017-01-trnted/texts/ko/en/._.eval\n","2017-01-trnted/texts/ko/en/.eval\n","2017-01-trnted/texts/ko/en/._.info\n","2017-01-trnted/texts/ko/en/.info\n","2017-01-trnted/texts/ko/en/._ko-en.tgz\n","2017-01-trnted/texts/ko/en/ko-en.tgz\n","2017-01-trnted/texts/ja/en/\n","2017-01-trnted/texts/ja/en/._.eval\n","2017-01-trnted/texts/ja/en/.eval\n","2017-01-trnted/texts/ja/en/._.info\n","2017-01-trnted/texts/ja/en/.info\n","2017-01-trnted/texts/ja/en/._ja-en.tgz\n","2017-01-trnted/texts/ja/en/ja-en.tgz\n","2017-01-trnted/texts/fr/en/\n","2017-01-trnted/texts/fr/en/._.eval\n","2017-01-trnted/texts/fr/en/.eval\n","2017-01-trnted/texts/fr/en/._.info\n","2017-01-trnted/texts/fr/en/.info\n","2017-01-trnted/texts/fr/en/._fr-en.tgz\n","2017-01-trnted/texts/fr/en/fr-en.tgz\n","2017-01-trnted/texts/en/ar/\n","2017-01-trnted/texts/en/de/\n","2017-01-trnted/texts/en/fr/\n","2017-01-trnted/texts/en/ja/\n","2017-01-trnted/texts/en/ko/\n","2017-01-trnted/texts/en/zh/\n","2017-01-trnted/texts/en/zh/._.eval\n","2017-01-trnted/texts/en/zh/.eval\n","2017-01-trnted/texts/en/zh/._.info\n","2017-01-trnted/texts/en/zh/.info\n","2017-01-trnted/texts/en/zh/._en-zh.tgz\n","2017-01-trnted/texts/en/zh/en-zh.tgz\n","2017-01-trnted/texts/en/ko/._.eval\n","2017-01-trnted/texts/en/ko/.eval\n","2017-01-trnted/texts/en/ko/._.info\n","2017-01-trnted/texts/en/ko/.info\n","2017-01-trnted/texts/en/ko/._en-ko.tgz\n","2017-01-trnted/texts/en/ko/en-ko.tgz\n","2017-01-trnted/texts/en/ja/._.eval\n","2017-01-trnted/texts/en/ja/.eval\n","2017-01-trnted/texts/en/ja/._.info\n","2017-01-trnted/texts/en/ja/.info\n","2017-01-trnted/texts/en/ja/._en-ja.tgz\n","2017-01-trnted/texts/en/ja/en-ja.tgz\n","2017-01-trnted/texts/en/fr/._.eval\n","2017-01-trnted/texts/en/fr/.eval\n","2017-01-trnted/texts/en/fr/._.info\n","2017-01-trnted/texts/en/fr/.info\n","2017-01-trnted/texts/en/fr/._en-fr.tgz\n","2017-01-trnted/texts/en/fr/en-fr.tgz\n","2017-01-trnted/texts/en/de/._.eval\n","2017-01-trnted/texts/en/de/.eval\n","2017-01-trnted/texts/en/de/._.info\n","2017-01-trnted/texts/en/de/.info\n","2017-01-trnted/texts/en/de/._en-de.tgz\n","2017-01-trnted/texts/en/de/en-de.tgz\n","2017-01-trnted/texts/en/ar/._.eval\n","2017-01-trnted/texts/en/ar/.eval\n","2017-01-trnted/texts/en/ar/._.info\n","2017-01-trnted/texts/en/ar/.info\n","2017-01-trnted/texts/en/ar/._en-ar.tgz\n","2017-01-trnted/texts/en/ar/en-ar.tgz\n","2017-01-trnted/texts/de/en/\n","2017-01-trnted/texts/de/en/._.eval\n","2017-01-trnted/texts/de/en/.eval\n","2017-01-trnted/texts/de/en/._.info\n","2017-01-trnted/texts/de/en/.info\n","2017-01-trnted/texts/de/en/._de-en.tgz\n","2017-01-trnted/texts/de/en/de-en.tgz\n","2017-01-trnted/texts/ar/en/\n","2017-01-trnted/texts/ar/en/._.eval\n","2017-01-trnted/texts/ar/en/.eval\n","2017-01-trnted/texts/ar/en/._.info\n","2017-01-trnted/texts/ar/en/.info\n","2017-01-trnted/texts/ar/en/._ar-en.tgz\n","2017-01-trnted/texts/ar/en/ar-en.tgz\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pHrN51fJ6cjJ"},"source":["The `texts` subdirectory contains translation data for multiple languages. Let's start with `fr-en`, French to English translations."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iAVpplSI6YW-","executionInfo":{"status":"ok","timestamp":1621427890587,"user_tz":0,"elapsed":6710,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}},"outputId":"5c9b80a1-7964-4eb8-e2dd-62aea5158ff8"},"source":["!tar -xvf 2017-01-trnted/texts/fr/en/fr-en.tgz"],"execution_count":7,"outputs":[{"output_type":"stream","text":["fr-en/\n","fr-en/IWSLT17.TED.dev2010.fr-en.en.xml\n","fr-en/IWSLT17.TED.dev2010.fr-en.fr.xml\n","fr-en/IWSLT17.TED.tst2010.fr-en.en.xml\n","fr-en/IWSLT17.TED.tst2010.fr-en.fr.xml\n","fr-en/IWSLT17.TED.tst2011.fr-en.en.xml\n","fr-en/IWSLT17.TED.tst2011.fr-en.fr.xml\n","fr-en/IWSLT17.TED.tst2012.fr-en.en.xml\n","fr-en/IWSLT17.TED.tst2012.fr-en.fr.xml\n","fr-en/IWSLT17.TED.tst2013.fr-en.en.xml\n","fr-en/IWSLT17.TED.tst2013.fr-en.fr.xml\n","fr-en/IWSLT17.TED.tst2014.fr-en.en.xml\n","fr-en/IWSLT17.TED.tst2014.fr-en.fr.xml\n","fr-en/IWSLT17.TED.tst2015.fr-en.en.xml\n","fr-en/IWSLT17.TED.tst2015.fr-en.fr.xml\n","fr-en/README\n","fr-en/train.en\n","fr-en/train.tags.fr-en.en\n","fr-en/train.tags.fr-en.fr\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"d5pNHfnZHnzb"},"source":["## Pre-processing"]},{"cell_type":"markdown","metadata":{"id":"pGjjucDo7ZtO"},"source":["The parallel data is stored in XML, see the description in the README. But it's multiple documents per file, so XML parsing requires splitting it. We'll go the quick and dirty way, as in this [pre-processing script](https://github.com/pytorch/fairseq/blob/master/examples/translation/prepare-iwslt14.sh) by just removing all metainformation that we're not interested in (i.e. every line containing a html tag. This is *not a good example for mindful pre-processing*, but good enough for now. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yjktf_ky6qgi","executionInfo":{"status":"ok","timestamp":1621427890592,"user_tz":0,"elapsed":3833,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}},"outputId":"e10e786b-3ac1-4e33-bedc-b085bf3abf82"},"source":["! head -n 20 /content/fr-en/train.tags.fr-en.en"],"execution_count":8,"outputs":[{"output_type":"stream","text":["<doc docid=\"1\" genre=\"lectures\"> \n","<url>http://www.ted.com/talks/al_gore_on_averting_climate_crisis</url> \n","<keywords>talks, alternative energy, cars, climate change, culture, environment, global issues, politics, science, sustainability, technology</keywords> \n","<speaker>Al Gore</speaker> \n","<talkid>1</talkid> \n","<title>Al Gore: Averting the climate crisis</title> \n","<description>TED Talk Subtitles and Transcript: With the same humor and humanity he exuded in \"An Inconvenient Truth,\" Al Gore spells out 15 ways that individuals can address climate change immediately, from buying a hybrid to inventing a new, hotter brand name for global warming.</description> \n","Thank you so much, Chris. And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful. \n","I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night. \n","And I say that sincerely, partly because  Put yourselves in my position. \n","I flew on Air Force Two for eight years. \n","Now I have to take off my shoes or boots to get on an airplane! \n"," I'll tell you one quick story to illustrate what that's been like for me. \n","It's a true story -- every bit of this is true. \n","Soon after Tipper and I left the --  White House --  we were driving from our home in Nashville to a little farm we have 50 miles east of Nashville. \n","Driving ourselves. \n","I know it sounds like a little thing to you, but -- I looked in the rear-view mirror and all of a sudden it just hit me. \n","There was no motorcade back there. \n","You've heard of phantom limb pain?  This was a rented Ford Taurus.  It was dinnertime, and we started looking for a place to eat. \n","We were on I-40. We got to Exit 238, Lebanon, Tennessee. \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UhXOSyKVBRae","executionInfo":{"status":"ok","timestamp":1621427892794,"user_tz":0,"elapsed":963,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["def remove_xml(filename):\n","  \"\"\"Remove all lines that contain xml brackets except for those in <seg>.\"\"\"\n","  valid_lines = []\n","  with open(filename, 'r') as ofile:\n","    for line in ofile:\n","      if ('<' in line or '>' in line) and not '<seg' in line:\n","        continue\n","      else:\n","        # Get content between <seg> tags for dev and test sets.\n","        if '<seg' in line:\n","          content = line.strip().split('>')[1].split('<')[0]\n","        else: \n","          content = line.strip()\n","        valid_lines.append(content)\n","  return valid_lines"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1N5gyBpz8jeW","executionInfo":{"status":"ok","timestamp":1621427896322,"user_tz":0,"elapsed":1709,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}},"outputId":"9c538c2b-9457-4b5e-9239-d2428b748d30"},"source":["targets = remove_xml('/content/fr-en/train.tags.fr-en.en')\n","print(f'Read {len(targets)} target sentences.')\n","    \n","sources = remove_xml('/content/fr-en/train.tags.fr-en.fr')\n","print(f'Read {len(sources)} source sentences.')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Read 232825 target sentences.\n","Read 232825 source sentences.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wNoL2WyF_rgD"},"source":["Let's check if they match."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xFgKyecW8nSP","executionInfo":{"status":"ok","timestamp":1621427898081,"user_tz":0,"elapsed":832,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}},"outputId":"72578997-400c-4b3a-90ff-07e40a194ffe"},"source":["num_examples = 3\n","for s, t in zip(sources[:num_examples], targets[:num_examples]):\n","  print(s)\n","  print(t)\n","  print()"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Merci beaucoup, Chris. C'est vraiment un honneur de pouvoir venir sur cette scène une deuxième fois. Je suis très reconnaissant.\n","Thank you so much, Chris. And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\n","\n","J'ai été très impressionné par cette conférence, et je tiens à vous remercier tous pour vos nombreux et sympathiques commentaires sur ce que j'ai dit l'autre soir.\n","I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.\n","\n","Et je dis çà sincèrement, en autres parce que --Faux sanglot-- j'en ai besoin ! --Rires-- Mettez-vous à ma place!\n","And I say that sincerely, partly because  Put yourselves in my position.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iais2WO6AIa5"},"source":["Looks good! You might already see that the translations are sometimes not very literal. Let's write them into file to feed them to Joey NMT."]},{"cell_type":"code","metadata":{"id":"2DwIq9Su_2Pl","executionInfo":{"status":"ok","timestamp":1621427901510,"user_tz":0,"elapsed":1636,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["def write_to_file(sentences, filename):\n","  \"\"\"Write sentences to file.\"\"\"\n","  with open(filename, 'w') as ofile:\n","    for sent in sentences:\n","      ofile.write(sent.strip()+'\\n')"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"7B7ZLZo2KyiS","executionInfo":{"status":"ok","timestamp":1621427902103,"user_tz":0,"elapsed":1004,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["data_dir = '/content/fr-en'"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"lQ-aWXlGHCvv","executionInfo":{"status":"ok","timestamp":1621427903858,"user_tz":0,"elapsed":1716,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["file_prefix = 'parallel_'"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"bL0rTleLHI5k","executionInfo":{"status":"ok","timestamp":1621427904388,"user_tz":0,"elapsed":1046,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["src_lang = 'fr'\n","trg_lang = 'en'"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ve2BE5mcBPuO","executionInfo":{"status":"ok","timestamp":1621427906312,"user_tz":0,"elapsed":1662,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["train_src_file = os.path.join(data_dir, file_prefix+'train.'+src_lang)\n","train_trg_file = os.path.join(data_dir, file_prefix+'train.'+trg_lang)\n","write_to_file(targets, train_trg_file)\n","write_to_file(sources, train_src_file)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IckyC5M9Cv3-"},"source":["Great, now we need a development and a test set. As development set we can pick any of the `tst` or `dev` files in the data we just downloaded (these were used for testing and evaluation for previous years). We'll go with `tst2015` for testing and `tst2014` for development."]},{"cell_type":"markdown","metadata":{"id":"oR2g1vWgFQLy"},"source":["**Question for you**: Is this choice important? How do you think selecting a different dev/test set could influence our findings?"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qO_zFPcbFOHT","executionInfo":{"status":"ok","timestamp":1621427907804,"user_tz":0,"elapsed":1150,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}},"outputId":"59ecab2d-30a5-4116-9c8a-8e9d1451b70f"},"source":["test_targets = remove_xml('/content/fr-en/IWSLT17.TED.tst2015.fr-en.en.xml')\n","print(f'Read {len(test_targets)} test target sentences.')\n","    \n","test_sources = remove_xml('/content/fr-en/IWSLT17.TED.tst2015.fr-en.fr.xml')\n","print(f'Read {len(test_sources)} test source sentences.')\n","\n","dev_targets = remove_xml('/content/fr-en/IWSLT17.TED.tst2014.fr-en.en.xml')\n","print(f'Read {len(dev_targets)} dev target sentences.')\n","    \n","dev_sources = remove_xml('/content/fr-en/IWSLT17.TED.tst2014.fr-en.fr.xml')\n","print(f'Read {len(dev_sources)} dev source sentences.')"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Read 1210 test target sentences.\n","Read 1210 test source sentences.\n","Read 1306 dev target sentences.\n","Read 1306 dev source sentences.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b_UAV5oiFpmu","executionInfo":{"status":"ok","timestamp":1621427910299,"user_tz":0,"elapsed":1491,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["dev_src_file = os.path.join(data_dir, file_prefix+'dev.'+src_lang)\n","dev_trg_file = os.path.join(data_dir, file_prefix+'dev.'+trg_lang)\n","test_src_file = os.path.join(data_dir, file_prefix+'test.'+src_lang)\n","test_trg_file = os.path.join(data_dir, file_prefix+'test.'+trg_lang)\n","\n","write_to_file(dev_targets, dev_trg_file)\n","write_to_file(dev_sources, dev_src_file)\n","write_to_file(test_targets, test_trg_file)\n","write_to_file(test_sources, test_src_file)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LOClab0XHhvh"},"source":["## Sub-words\n","\n","Same procedure as in Lab 1."]},{"cell_type":"code","metadata":{"id":"5R_1HRBvHBn-","executionInfo":{"status":"ok","timestamp":1621427911928,"user_tz":0,"elapsed":1222,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["bpe_size = 4000"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"XDxhoKCuKnPE","executionInfo":{"status":"ok","timestamp":1621427912721,"user_tz":0,"elapsed":904,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["train_joint_file = os.path.join(data_dir, file_prefix+'train.'+src_lang+'-'+trg_lang)\n","\n","src_files = {'train': train_src_file, 'dev': dev_src_file, 'test': test_src_file}\n","trg_files = {'train': train_trg_file, 'dev': dev_trg_file, 'test': test_trg_file}\n","\n","vocab_src_file = os.path.join(data_dir, f'vocab.{bpe_size}.{src_lang}')\n","vocab_trg_file = os.path.join(data_dir, f'vocab.{bpe_size}.{trg_lang}')\n","bpe_file = os.path.join(data_dir, f'bpe.codes.{bpe_size}')"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"q9OpCuhsKpIZ","executionInfo":{"status":"ok","timestamp":1621427942983,"user_tz":0,"elapsed":5,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["! cat $train_src_file $train_trg_file > $train_joint_file\n","\n","! subword-nmt learn-bpe \\\n","  --input $train_joint_file \\\n","  -s $bpe_size \\\n","  -o $bpe_file"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"nZiHb1iVKsWS","executionInfo":{"status":"ok","timestamp":1621427968105,"user_tz":0,"elapsed":50544,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["src_bpe_files = {}\n","trg_bpe_files = {}\n","for split in ['train', 'dev', 'test']:\n","  src_input_file = src_files[split]\n","  trg_input_file = trg_files[split]\n","  src_output_file = src_input_file.replace(split, f'{split}.{bpe_size}.bpe')\n","  trg_output_file = trg_input_file.replace(split, f'{split}.{bpe_size}.bpe')\n","  src_bpe_files[split] = src_output_file\n","  trg_bpe_files[split] = trg_output_file\n","\n","  ! subword-nmt apply-bpe \\\n","    -c $bpe_file \\\n","    < $src_input_file > $src_output_file\n","\n","  ! subword-nmt apply-bpe \\\n","    -c $bpe_file \\\n","    < $trg_input_file > $trg_output_file\n"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d3TMw58tKu5M","executionInfo":{"status":"ok","timestamp":1621427968714,"user_tz":0,"elapsed":46784,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}},"outputId":"4b64d925-41c7-409f-cbed-60fd6e8f9e2b"},"source":["! wget https://raw.githubusercontent.com/joeynmt/joeynmt/master/scripts/build_vocab.py"],"execution_count":23,"outputs":[{"output_type":"stream","text":["--2021-05-19 12:39:27--  https://raw.githubusercontent.com/joeynmt/joeynmt/master/scripts/build_vocab.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2034 (2.0K) [text/plain]\n","Saving to: ‘build_vocab.py’\n","\n","build_vocab.py      100%[===================>]   1.99K  --.-KB/s    in 0s      \n","\n","2021-05-19 12:39:28 (36.5 MB/s) - ‘build_vocab.py’ saved [2034/2034]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2IzclnElKwkg","executionInfo":{"status":"ok","timestamp":1621427972512,"user_tz":0,"elapsed":48330,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["vocab_src_file = src_bpe_files['train']\n","vocab_trg_file = trg_bpe_files['train']\n","bpe_vocab_file = os.path.join(data_dir, f'joint.{bpe_size}bpe.vocab')\n","\n","! python build_vocab.py  \\\n","  $vocab_src_file $vocab_trg_file \\\n","  --output_path $bpe_vocab_file"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U92TAIbLO0vV"},"source":["# Training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XOswhM_cTkVu","executionInfo":{"status":"ok","timestamp":1621428009646,"user_tz":0,"elapsed":32871,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}},"outputId":"07bda16f-4a04-4852-b0f6-38699405274e"},"source":["from google.colab import drive\n","drive_home = '/content/drive'\n","drive.mount(drive_home)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IoiHXv_STskk","executionInfo":{"status":"ok","timestamp":1621428011338,"user_tz":0,"elapsed":1679,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["g_drive_path = \"/content/drive/My\\ Drive/NMT_Lab2/models/%s-%s\" % (src_lang, trg_lang)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"mfX4_9cGTXHL","executionInfo":{"status":"ok","timestamp":1621428011339,"user_tz":0,"elapsed":1674,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["experiment_name = 'ted_fr_en'"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"dJh0MywhTzhq","executionInfo":{"status":"ok","timestamp":1621428011341,"user_tz":0,"elapsed":1671,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["model_path = os.path.join(g_drive_path, experiment_name)"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dLm8TR2LFbVX"},"source":["Copy the BPE merges to Gdrive so we don't lose them."]},{"cell_type":"code","metadata":{"id":"hoLrEKP9FY5e","executionInfo":{"status":"ok","timestamp":1621428014663,"user_tz":0,"elapsed":3388,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["bpe_drive_path = \"/content/drive/My\\ Drive/NMT_Lab2/bpe/%s-%s\" % (src_lang, trg_lang)\n","! mkdir -p $bpe_drive_path\n","! cp $bpe_file $bpe_drive_path "],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FvYBPK2nO2us"},"source":["**TODO:**\n","\n","The following configuration file contains *three* bugs that prevent it from working (=quickly giving reasonable BLEU for translating between French and English). Find and fix those three. Try not to compare with the config from Lab 1 ;)"]},{"cell_type":"code","metadata":{"id":"cG6ftq_2PVIu","executionInfo":{"status":"ok","timestamp":1621429700701,"user_tz":0,"elapsed":4287,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["# Create the config\n","broken_config = \"\"\"\n","name: \"{name}\"\n","\n","data:\n","    src: \"{source_language}\"\n","    trg: \"{target_language}\"\n","    train: \"{data_dir}/parallel_train.{bpe_size}.bpe\"  \n","    dev:   \"{data_dir}/parallel_dev.{bpe_size}.bpe\"\n","    test:  \"{data_dir}/parallel_test.{bpe_size}.bpe\"\n","    level: \"bpe\"                   # Here we specify we're working on BPEs.\n","    lowercase: False                \n","    max_sent_length: 30             # Extend to longer sentences.\n","    src_vocab: \"{src_vocab_path}\"\n","    trg_vocab: \"{trg_vocab_path}\"\n","\n","testing:\n","    beam_size: 5\n","    alpha: 1.0\n","    sacrebleu:                      # sacrebleu options\n","        remove_whitespace: True     # `remove_whitespace` option in sacrebleu.corpus_chrf() function (defalut: True)\n","        tokenize: \"intl\"            # `tokenize` option in sacrebleu.corpus_bleu() function (options include: \"none\" (use for already tokenized test data), \"13a\" (default minimal tokenizer), \"intl\" which mostly does punctuation and unicode, etc) \n","\n","training:\n","    #load_model: \"{model_path}/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n","    random_seed: 42\n","    optimizer: \"adam\"\n","    normalization: \"tokens\"\n","    adam_betas: [0.9, 0.999] \n","    scheduling: \"plateau\"           # Alternative: try switching from plateau to Noam scheduling\n","    patience: 30                    # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n","    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n","    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n","    decrease_factor: 0.7\n","    loss: \"crossentropy\"\n","    learning_rate: 0.00001\n","    learning_rate_min: 0.00000001\n","    weight_decay: 0.0\n","    label_smoothing: 0.1\n","    batch_size: 4096\n","    batch_type: \"token\"\n","    eval_batch_size: 3600\n","    eval_batch_type: \"token\"\n","    batch_multiplier: 1\n","    early_stopping_metric: \"ppl\"\n","    epochs: 5                     # Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n","    validation_freq: 500          # Set to at least once per epoch.\n","    logging_freq: 100\n","    eval_metric: \"bleu\"\n","    model_dir: \"{model_path}\"\n","    overwrite: True                 # Set to True if you want to overwrite possibly existing models. \n","    shuffle: True\n","    use_cuda: True\n","    max_output_length: 100\n","    print_valid_sents: [0, 1, 2, 3]\n","    keep_last_ckpts: 3\n","\n","model:\n","    initializer: \"xavier\"\n","    bias_initializer: \"zeros\"\n","    init_gain: 1.0\n","    embed_initializer: \"xavier\"\n","    embed_init_gain: 1.0\n","    tied_embeddings: True        # Joint vocabulary.\n","    tied_softmax: True\n","    encoder:\n","        type: \"transformer\"\n","        num_layers: 6\n","        num_heads: 4             # Increase to 8 for larger data.\n","        embeddings:\n","            embedding_dim: 256   # Increase to 512 for larger data.\n","            scale: True\n","            dropout: 0.2\n","        # typically ff_size = 4 x hidden_size\n","        hidden_size: 256         # Increase to 512 for larger data.\n","        ff_size: 1024            # Increase to 2048 for larger data.\n","        dropout: 0.3\n","    decoder:\n","        type: \"transformer\"\n","        num_layers: 6\n","        num_heads: 4              # Increase to 8 for larger data.\n","        embeddings:\n","            embedding_dim: 256    # Increase to 512 for larger data.\n","            scale: True\n","            dropout: 0.2\n","        # typically ff_size = 4 x hidden_size\n","        hidden_size: 256         # TODO: Increase to 512 for larger data.\n","        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n","        dropout: 0.3\n","\"\"\".format(name=experiment_name, \n","           source_language=src_lang, \n","           target_language=trg_lang,\n","           data_dir=data_dir, \n","           model_path=model_path, \n","           src_vocab_path=bpe_vocab_file,\n","           trg_vocab_path=bpe_vocab_file, \n","           bpe_size=bpe_size)\n","with open(\"transformer_{name}.yaml\".format(name=experiment_name),'w') as f:\n","    f.write(broken_config)"],"execution_count":53,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rF14SqV0UbfM"},"source":["If you try running this training multiple times for debugging, set `overwrite` to `True` in the config."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NNILzUckUFFv","outputId":"2ef34ca0-570d-4291-8267-485e5663d623"},"source":["!python -m joeynmt train transformer_ted_fr_en.yaml"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-05-19 13:08:18,404 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n","2021-05-19 13:08:18,420 - INFO - joeynmt.data - Loading training data...\n","2021-05-19 13:08:22,881 - INFO - joeynmt.data - Building vocabulary...\n","2021-05-19 13:08:23,115 - INFO - joeynmt.data - Loading dev data...\n","2021-05-19 13:08:23,138 - INFO - joeynmt.data - Loading test data...\n","2021-05-19 13:08:23,150 - INFO - joeynmt.data - Data loaded.\n","2021-05-19 13:08:23,151 - INFO - joeynmt.model - Building an encoder-decoder model...\n","2021-05-19 13:08:23,357 - INFO - joeynmt.model - Enc-dec model built.\n","2021-05-19 13:08:23.474453: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","2021-05-19 13:08:25,112 - INFO - joeynmt.training - Total params: 12170752\n","2021-05-19 13:08:27,579 - INFO - joeynmt.helpers - cfg.name                           : ted_fr_en\n","2021-05-19 13:08:27,579 - INFO - joeynmt.helpers - cfg.data.src                       : fr\n","2021-05-19 13:08:27,579 - INFO - joeynmt.helpers - cfg.data.trg                       : en\n","2021-05-19 13:08:27,579 - INFO - joeynmt.helpers - cfg.data.train                     : /content/fr-en/parallel_train.4000.bpe\n","2021-05-19 13:08:27,579 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/fr-en/parallel_dev.4000.bpe\n","2021-05-19 13:08:27,579 - INFO - joeynmt.helpers - cfg.data.test                      : /content/fr-en/parallel_test.4000.bpe\n","2021-05-19 13:08:27,579 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n","2021-05-19 13:08:27,580 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n","2021-05-19 13:08:27,580 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 30\n","2021-05-19 13:08:27,580 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/fr-en/joint.4000bpe.vocab\n","2021-05-19 13:08:27,580 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/fr-en/joint.4000bpe.vocab\n","2021-05-19 13:08:27,580 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n","2021-05-19 13:08:27,580 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n","2021-05-19 13:08:27,581 - INFO - joeynmt.helpers - cfg.testing.sacrebleu.remove_whitespace : True\n","2021-05-19 13:08:27,581 - INFO - joeynmt.helpers - cfg.testing.sacrebleu.tokenize     : intl\n","2021-05-19 13:08:27,581 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n","2021-05-19 13:08:27,581 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n","2021-05-19 13:08:27,581 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n","2021-05-19 13:08:27,581 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n","2021-05-19 13:08:27,582 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n","2021-05-19 13:08:27,582 - INFO - joeynmt.helpers - cfg.training.patience              : 30\n","2021-05-19 13:08:27,582 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n","2021-05-19 13:08:27,582 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n","2021-05-19 13:08:27,582 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n","2021-05-19 13:08:27,582 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n","2021-05-19 13:08:27,582 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 1e-05\n","2021-05-19 13:08:27,582 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n","2021-05-19 13:08:27,583 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n","2021-05-19 13:08:27,583 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n","2021-05-19 13:08:27,583 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n","2021-05-19 13:08:27,583 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n","2021-05-19 13:08:27,583 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n","2021-05-19 13:08:27,583 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n","2021-05-19 13:08:27,583 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n","2021-05-19 13:08:27,583 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n","2021-05-19 13:08:27,584 - INFO - joeynmt.helpers - cfg.training.epochs                : 5\n","2021-05-19 13:08:27,584 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 500\n","2021-05-19 13:08:27,584 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n","2021-05-19 13:08:27,584 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n","2021-05-19 13:08:27,584 - INFO - joeynmt.helpers - cfg.training.model_dir             : /content/drive/My Drive/NMT_Lab2/models/fr-en/ted_fr_en\n","2021-05-19 13:08:27,584 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n","2021-05-19 13:08:27,584 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n","2021-05-19 13:08:27,585 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n","2021-05-19 13:08:27,585 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n","2021-05-19 13:08:27,585 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n","2021-05-19 13:08:27,585 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n","2021-05-19 13:08:27,585 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n","2021-05-19 13:08:27,585 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n","2021-05-19 13:08:27,585 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n","2021-05-19 13:08:27,586 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n","2021-05-19 13:08:27,586 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n","2021-05-19 13:08:27,586 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n","2021-05-19 13:08:27,586 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n","2021-05-19 13:08:27,586 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n","2021-05-19 13:08:27,586 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n","2021-05-19 13:08:27,586 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n","2021-05-19 13:08:27,586 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n","2021-05-19 13:08:27,587 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n","2021-05-19 13:08:27,587 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n","2021-05-19 13:08:27,587 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n","2021-05-19 13:08:27,587 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n","2021-05-19 13:08:27,587 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n","2021-05-19 13:08:27,587 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n","2021-05-19 13:08:27,587 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n","2021-05-19 13:08:27,587 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n","2021-05-19 13:08:27,588 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n","2021-05-19 13:08:27,588 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n","2021-05-19 13:08:27,588 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n","2021-05-19 13:08:27,588 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n","2021-05-19 13:08:27,588 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n","2021-05-19 13:08:27,588 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n","2021-05-19 13:08:27,588 - INFO - joeynmt.helpers - Data set sizes: \n","\ttrain 148618,\n","\tvalid 1306,\n","\ttest 1210\n","2021-05-19 13:08:27,588 - INFO - joeynmt.helpers - First training example:\n","\t[SRC] J'ai vol@@ é avec A@@ ir For@@ ce 2 pendant hu@@ it ans.\n","\t[TRG] I f@@ le@@ w on A@@ ir For@@ ce T@@ w@@ o for ei@@ ght years.\n","2021-05-19 13:08:27,589 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) de (6) a (7) to (8) of (9) and\n","2021-05-19 13:08:27,589 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) de (6) a (7) to (8) of (9) and\n","2021-05-19 13:08:27,589 - INFO - joeynmt.helpers - Number of Src words (types): 4338\n","2021-05-19 13:08:27,589 - INFO - joeynmt.helpers - Number of Trg words (types): 4338\n","2021-05-19 13:08:27,590 - INFO - joeynmt.training - Model(\n","\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n","\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n","\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4338),\n","\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4338))\n","2021-05-19 13:08:27,600 - INFO - joeynmt.training - Train stats:\n","\tdevice: cuda\n","\tn_gpu: 1\n","\t16-bits training: False\n","\tgradient accumulation: 1\n","\tbatch size per device: 4096\n","\ttotal batch size (w. parallel & accumulation): 4096\n","2021-05-19 13:08:27,601 - INFO - joeynmt.training - EPOCH 1\n","2021-05-19 13:08:38,476 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     6.887353, Tokens per Sec:    23222, Lr: 0.000010\n","2021-05-19 13:08:49,072 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     6.695253, Tokens per Sec:    23931, Lr: 0.000010\n","2021-05-19 13:08:59,727 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     6.342000, Tokens per Sec:    24081, Lr: 0.000010\n","2021-05-19 13:09:10,346 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     6.287025, Tokens per Sec:    23943, Lr: 0.000010\n","2021-05-19 13:09:20,975 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     6.123258, Tokens per Sec:    23955, Lr: 0.000010\n","2021-05-19 13:09:24,033 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n","2021-05-19 13:09:24,033 - INFO - joeynmt.training - Saving new checkpoint.\n","2021-05-19 13:09:24,654 - INFO - joeynmt.training - Example #0\n","2021-05-19 13:09:24,656 - INFO - joeynmt.training - \tSource:     Bonjour, TEDWomen, ça va ?\n","2021-05-19 13:09:24,656 - INFO - joeynmt.training - \tReference:  Hello, TEDWomen, what's up.\n","2021-05-19 13:09:24,656 - INFO - joeynmt.training - \tHypothesis: \n","2021-05-19 13:09:24,656 - INFO - joeynmt.training - Example #1\n","2021-05-19 13:09:24,657 - INFO - joeynmt.training - \tSource:     C'est pas encore ça.\n","2021-05-19 13:09:24,657 - INFO - joeynmt.training - \tReference:  Not good enough.\n","2021-05-19 13:09:24,657 - INFO - joeynmt.training - \tHypothesis: \n","2021-05-19 13:09:24,657 - INFO - joeynmt.training - Example #2\n","2021-05-19 13:09:24,657 - INFO - joeynmt.training - \tSource:     Bonjour, TEDWomen, comment ça va ?\n","2021-05-19 13:09:24,658 - INFO - joeynmt.training - \tReference:  Hello, TEDWomen, what is up?\n","2021-05-19 13:09:24,658 - INFO - joeynmt.training - \tHypothesis: \n","2021-05-19 13:09:24,658 - INFO - joeynmt.training - Example #3\n","2021-05-19 13:09:24,658 - INFO - joeynmt.training - \tSource:     Je m'appelle Maysoon Zayid, et je ne suis pas saoule, mais le docteur qui m'a fait naître l'était.\n","2021-05-19 13:09:24,658 - INFO - joeynmt.training - \tReference:  My name is Maysoon Zayid, and I am not drunk, but the doctor who delivered me was.\n","2021-05-19 13:09:24,659 - INFO - joeynmt.training - \tHypothesis: \n","2021-05-19 13:09:24,659 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step      500: bleu:   0.00, loss: 214167.9219, ppl: 523.9980, duration: 3.6843s\n","2021-05-19 13:09:35,607 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     6.141086, Tokens per Sec:    23554, Lr: 0.000010\n","2021-05-19 13:09:46,274 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     6.010542, Tokens per Sec:    23962, Lr: 0.000010\n","2021-05-19 13:09:56,887 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     5.964285, Tokens per Sec:    23949, Lr: 0.000010\n","2021-05-19 13:10:07,522 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     5.952206, Tokens per Sec:    23942, Lr: 0.000010\n","2021-05-19 13:10:14,426 - INFO - joeynmt.training - Epoch   1: total training loss 6059.27\n","2021-05-19 13:10:14,427 - INFO - joeynmt.training - EPOCH 2\n","2021-05-19 13:10:18,362 - INFO - joeynmt.training - Epoch   2, Step:     1000, Batch Loss:     5.845055, Tokens per Sec:    22757, Lr: 0.000010\n","2021-05-19 13:10:23,432 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n","2021-05-19 13:10:23,433 - INFO - joeynmt.training - Saving new checkpoint.\n","2021-05-19 13:10:24,138 - INFO - joeynmt.training - Example #0\n","2021-05-19 13:10:24,139 - INFO - joeynmt.training - \tSource:     Bonjour, TEDWomen, ça va ?\n","2021-05-19 13:10:24,139 - INFO - joeynmt.training - \tReference:  Hello, TEDWomen, what's up.\n","2021-05-19 13:10:24,139 - INFO - joeynmt.training - \tHypothesis: \n","2021-05-19 13:10:24,139 - INFO - joeynmt.training - Example #1\n","2021-05-19 13:10:24,140 - INFO - joeynmt.training - \tSource:     C'est pas encore ça.\n","2021-05-19 13:10:24,140 - INFO - joeynmt.training - \tReference:  Not good enough.\n","2021-05-19 13:10:24,140 - INFO - joeynmt.training - \tHypothesis: \n","2021-05-19 13:10:24,140 - INFO - joeynmt.training - Example #2\n","2021-05-19 13:10:24,141 - INFO - joeynmt.training - \tSource:     Bonjour, TEDWomen, comment ça va ?\n","2021-05-19 13:10:24,141 - INFO - joeynmt.training - \tReference:  Hello, TEDWomen, what is up?\n","2021-05-19 13:10:24,141 - INFO - joeynmt.training - \tHypothesis: And\n","2021-05-19 13:10:24,141 - INFO - joeynmt.training - Example #3\n","2021-05-19 13:10:24,141 - INFO - joeynmt.training - \tSource:     Je m'appelle Maysoon Zayid, et je ne suis pas saoule, mais le docteur qui m'a fait naître l'était.\n","2021-05-19 13:10:24,142 - INFO - joeynmt.training - \tReference:  My name is Maysoon Zayid, and I am not drunk, but the doctor who delivered me was.\n","2021-05-19 13:10:24,142 - INFO - joeynmt.training - \tHypothesis: And the the the the the the the the the the\n","2021-05-19 13:10:24,142 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     1000: bleu:   0.01, loss: 204579.5625, ppl: 395.8990, duration: 5.7794s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"w3D6LbFGWV_7"},"source":["If correct, the model should obtain roughly the following BLEU scores (or better!):\n","\n","\n","*   Step 500: 0.05\n","*   Step 1000: 1.24\n","*   Step 1500: 1.97\n","*   Step 2000: 3.71\n","*   ...\n","*   Step 3000: 7.12\n","*   Step 8000: 15.30\n","*   Step 17000: 21.69\n","*   Step 27000: 23.67  (around 2h training time)\n","\n","You don't need to wait that long for the purpose of this exercise, Julia can provide a checkpoint for an already trained model :)\n"]},{"cell_type":"markdown","metadata":{"id":"s8DIatjWHxQk"},"source":["**TODO:**\n","1. Why does the number of source words reported in the log not match the specified number of BPE merges? Tip: browse the [subword-nmt GitHub](https://github.com/rsennrich/subword-nmt).\n","2. Imagine your job is to provide the best translation system as soon as possible. Try changing a few hyperparameters to see what the best score is that you can get within three epochs of training. You may also coordinate this with your colleagues.\n","  * Suggestions: try changing bpe size, learning rate, batch size. \n","  * Recommendation: create a new configuration and experiment directory for each experiment so you can tell them apart.\n","  * You can spend endless time on this, but try to select a few settings that you'd hope could improve the result. \n","  * Do you observe any tendency? Compare with your colleagues.\n"]},{"cell_type":"markdown","metadata":{"id":"cxFq5xUbBZrt"},"source":["*Notes*"]},{"cell_type":"markdown","metadata":{"id":"D3d3xwjVCJ3n"},"source":["\n","# Testing"]},{"cell_type":"markdown","metadata":{"id":"mQpIPDeKYpFA"},"source":["For the following exercises you may either use your own model or the trained one provided by Julia (trained for 30 epochs).\n","\n","Now that we got a trained model, let's see how well it does. We'll probe for the following examples:\n","\n","1. A *training/memorization/overfitting* check: Did model learn to perfectly translate the training set?\n","2. Unseen but from the *same domain*: Did the model learn to generalize to unseen examples?\n","3. *Out-of-domain*: Can the model translate a random sentence from the source language?\n","\n","It will be increasingly hard for the model to do well on these. But even in the training set you can probably find outliers that the model does not translate well."]},{"cell_type":"code","metadata":{"id":"qJDr8Fw3EONl","executionInfo":{"status":"ok","timestamp":1621349237931,"user_tz":0,"elapsed":1870,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["from joeynmt.helpers import load_config\n","import yaml\n","\n","\n","def download_pretrained_model_from_gdrive(\n","    checkpoint='1sIaNogftpt-moKEKRBMAKbE4QdOb7XwM',\n","    config='1_FpHfRn8bxLu_pAUgj99jtZuBcCABgXV',\n","    src_vocabulary='1esULLiG-2fS6Ucj2LMndUoID8WaY_QuZ',\n","    trg_vocabulary='1sdygCZxK6h8M1khDlh-TLAq0Y4DNczl_',\n","    bpe_merges='17XeygY048oXQHHzmH4u_hiJl1GndkiSN',\n","    directory='/content/pretrained_model'):\n","  \n","  \"\"\"Download pretrained model from ids and place it in given directory. \n","  Adjust paths in config as needed.\n","  Default ids are for a model as specified above, but trained for the full\n","  30 epochs. \n","  \"\"\"\n","\n","  # Download files and place them into the new directory.\n","  original_config = os.path.join(directory, 'original_config.yaml')\n","  new_config = os.path.join(directory, 'config.yaml')\n","  checkpoint_path = os.path.join(directory, 'best.ckpt')\n","  trg_vocab_path = os.path.join(directory, 'trg_vocab.txt')\n","  src_vocab_path = os.path.join(directory, 'src_vocab.txt')\n","  bpe_path = os.path.join(directory, 'bpe.merges')\n","\n","  def gdown_by_id(id, output):\n","    ! gdown 'https://drive.google.com/uc?id='$id -O $output\n","\n","  ! mkdir -p $directory\n","  gdown_by_id(checkpoint, checkpoint_path)\n","  gdown_by_id(config, original_config)\n","  gdown_by_id(src_vocabulary, src_vocab_path)\n","  gdown_by_id(trg_vocabulary, trg_vocab_path)\n","  gdown_by_id(bpe_merges, bpe_path)\n","\n","  # Overwrite paths in config.\n","  config = load_config(original_config)\n","  config['data']['src_vocab'] = src_vocab_path\n","  config['data']['trg_vocab'] = trg_vocab_path\n","  config['training']['model_dir'] = directory\n","  config['training']['load_model'] = checkpoint_path\n","  with open(new_config, 'w') as cfile:\n","    yaml.dump(config, cfile)\n","  return new_config, bpe_path\n"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CYaF8k1cDA9G","executionInfo":{"status":"ok","timestamp":1621349249270,"user_tz":0,"elapsed":8308,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}},"outputId":"e0a66580-a1cb-4a88-9f5f-bd8b0864fc47"},"source":["# Download a pretrained model.\n","pretrained_config, pretrained_bpe = download_pretrained_model_from_gdrive()"],"execution_count":34,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1sIaNogftpt-moKEKRBMAKbE4QdOb7XwM\n","To: /content/pretrained_model/best.ckpt\n","157MB [00:02, 57.5MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1_FpHfRn8bxLu_pAUgj99jtZuBcCABgXV\n","To: /content/pretrained_model/original_config.yaml\n","100% 3.70k/3.70k [00:00<00:00, 5.78MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1esULLiG-2fS6Ucj2LMndUoID8WaY_QuZ\n","To: /content/pretrained_model/src_vocab.txt\n","100% 26.3k/26.3k [00:00<00:00, 1.75MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1sdygCZxK6h8M1khDlh-TLAq0Y4DNczl_\n","To: /content/pretrained_model/trg_vocab.txt\n","100% 26.3k/26.3k [00:00<00:00, 3.83MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=17XeygY048oXQHHzmH4u_hiJl1GndkiSN\n","To: /content/pretrained_model/bpe.merges\n","100% 33.8k/33.8k [00:00<00:00, 55.1MB/s]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dYRhVqjMH4RU"},"source":["**TODO:**\n","\n","\n","1.   Pick 2-5 sentences each from the three sets described above and translate them with your model in `translate` mode. Remember that you need to split them into BPEs first (already done for 1 and 2; example code for that in Lab 1).\n","2.   Compare their translations: Can you tell from these examples what kind of data the model was trained on? Anything surprisingly good or bad?\n","3. Choose one sentence that the model translated really well. Can you perturb it so that it's still very similar to the original but the translation is very different or significantly worse? \n","\n","Small changes in the input leading to small changes in the output can be seen as a criterion for robustness. The harder it is to find these adversarial inputs, the more robust is the model.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7rIihpclD5tm","executionInfo":{"status":"ok","timestamp":1621349564164,"user_tz":0,"elapsed":216836,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}},"outputId":"0810e95e-f439-4e56-bed7-ef07c0a37eed"},"source":["# Either use $pretrained_config for the pretrained model or your own trained model.\n","!python -m joeynmt translate $pretrained_config"],"execution_count":35,"outputs":[{"output_type":"stream","text":["2021-05-18 14:49:08,811 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n","2021-05-18 14:49:12,731 - INFO - joeynmt.model - Building an encoder-decoder model...\n","2021-05-18 14:49:12,934 - INFO - joeynmt.model - Enc-dec model built.\n","\n","Please enter a source sentence (pre-processed): \n","bonjour comment tu vas\n","JoeyNMT: Hypotheses ranked by score\n","JoeyNMT #1: How do you do that?\n","\n","Please enter a source sentence (pre-processed): \n","salut\n","JoeyNMT: Hypotheses ranked by score\n","JoeyNMT #1: We have to do that.\n","\n","Please enter a source sentence (pre-processed): \n","je pense que tu es malade\n","JoeyNMT: Hypotheses ranked by score\n","JoeyNMT #1: I think you are.\n","\n","Please enter a source sentence (pre-processed): \n","stop\n","JoeyNMT: Hypotheses ranked by score\n","JoeyNMT #1: stop there.\n","\n","Please enter a source sentence (pre-processed): \n","c'est fini\n","JoeyNMT: Hypotheses ranked by score\n","JoeyNMT #1: That's it.\n","\n","Please enter a source sentence (pre-processed): \n","quit\n","JoeyNMT: Hypotheses ranked by score\n","JoeyNMT #1: We have to do that.\n","\n","Please enter a source sentence (pre-processed): \n","c'est bon\n","JoeyNMT: Hypotheses ranked by score\n","JoeyNMT #1: That's good.\n","\n","Please enter a source sentence (pre-processed): \n","autement dit je vais te laisser\n","JoeyNMT: Hypotheses ranked by score\n","JoeyNMT #1: I said, \"I'm going to give you a\n","\n","Please enter a source sentence (pre-processed): \n","\n","Bye.\n","^C\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x2B83tnyDopt"},"source":["*Notes:*"]},{"cell_type":"code","metadata":{"id":"7YHLvzPr9Ah3"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d1yq7rq0I1UF"},"source":["# Evaluation"]},{"cell_type":"markdown","metadata":{"id":"hG1UC2rfKewE"},"source":["We now got an intuition of what the model can do and where its limits are. During validation, we trust the BLEU score to tell us whether the model is progressing.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cdPc2LjGK3-S"},"source":["1.   Compute the `sacrebleu` ([GitHub](https://github.com/mjpost/sacrebleu)) score for the dev set translations for a chosen step that are stored in your model directory (`.hyps`). Below is an example call. Do they match with the result that was reported in `validations.txt` and `train.log`?\n","2.   In the configuration we chose one particular tokenizer, but there are other options (hint: explore sacrebleu documentation). Does the reported score change? If so, why do you think this happens?\n","3. The `sacrebleu` library also implements the ChrF score. Compute the ChrF as well as the BLEU score for two validation steps. How do differ with respect to ChrF and BLEU, are the differences comparable?\n","\n","(We did not tokenize our data before feeding it to the model. Do you think it makes a difference? You can try it out with the `sacremoses` library that implements tokenizers.)"]},{"cell_type":"code","metadata":{"id":"wHkNRNIrUHVP","executionInfo":{"status":"ok","timestamp":1621349829325,"user_tz":0,"elapsed":1010,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["# Helper function\n","def read_sentences(inputfile):\n","  \"\"\"Read sentences from file into list.\"\"\"\n","  lines = []\n","  with open(inputfile, 'r') as ofile:\n","    for line in ofile:\n","      lines.append(line.strip())\n","  print(f'Read {len(lines)} sentences from file {inputfile}.')\n","  return lines"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"id":"e13-fIRnUogy"},"source":["# Model outputs\n","hyps = read_sentences('path/to/.hyps')\n","# And references for the same dev set\n","refs = read_sentences('path/to/references')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V9q9lYJnRwFY"},"source":["sacrebleu.corpus_bleu(hyps, [refs])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fk_0TAeRU-29"},"source":["Note the one-element list that we're passing to the BLEU score calculation. This is because BLEU was originally proposed to compute quality scores relative to multiple translations. However, in practice there are rarely multiple translations available, so we got to work with what we have."]},{"cell_type":"markdown","metadata":{"id":"JCzlKnCZKs1j"},"source":["# Extra: Backtranslation & Multilingual"]},{"cell_type":"markdown","metadata":{"id":"2VhSYBwNRcVM"},"source":["These experiments take more time than you'll have in the lab and relate to contents covered later this week. They might be interesting to explore if you want to keep learning about NMT :)"]},{"cell_type":"markdown","metadata":{"id":"MX9_V4veK7fM"},"source":["### Backtranslation"]},{"cell_type":"markdown","metadata":{"id":"jRq7DZ4FYhgE"},"source":["The downloaded data also contains a `train.en` file: monolingual data for English. This can be used to improve our model with backtranslation.  There are multiple steps and options involved:\n","  * First, you need to train a en-fr model.\n","  * Then use this reverse model to translate this monolingual data (or a part thereof, depending on translation speed).\n","  * Now you have synthetic training data that you can either 1) mix with the original training data as it is, 2) mix with a certain ratio, since this data has probably lower quality. \n","  * You can then either 1) further train the original `fr-en` model on this data, or 2) retrain a `fr-en` model to see if it gets better than the original data.\n"]},{"cell_type":"markdown","metadata":{"id":"McyAT7BPYidz"},"source":["*Notes:*"]},{"cell_type":"markdown","metadata":{"id":"9Q7Do7l1K_JF"},"source":["### Multilingual"]},{"cell_type":"markdown","metadata":{"id":"6jsgyXYDLCuG"},"source":["The downloaded directory also contains for other languages paired with English on the target side: `ar`, `de`, `ja`, `ko`, `zh`. Additional training data from other languages often helps to improve translation quality for small training data. \n","\n","We'll try out the \"many-to-one\" approach here: learning to translate from many languages into English. For the opposite, we would need to add special target language tags to the source (.e.g. `<2fr>`, `<2ja>` to tell the model which language it should translate into.\n","\n","* First, select one or more language pairs to add to fr-en.\n","* Repeat the pre-processing pipeline for them. Training and dev sets should get concatenated for joint training. BPE training should also be done on a concatenation of the training sets for all languages, so that the sub-word merges reflect all languages.\n","* Depending on the number of languages, your concatenated dev set might grow too large for regular validation during training, so you can also just take a smaller subset from each language and combine them. \n","* Do you find improvements over the original model? For a direct comparison you would need to translate the original fr-en dev or test set with the multilingual model (not the concatenated ones used for this experiment) and compare the scores.\n"]},{"cell_type":"markdown","metadata":{"id":"deylviR3RZGI"},"source":["*Notes:*"]},{"cell_type":"code","metadata":{"id":"UstIWIwKRaVp"},"source":[""],"execution_count":null,"outputs":[]}]}