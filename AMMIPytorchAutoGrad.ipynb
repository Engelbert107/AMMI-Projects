{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of AMMI - Pytorch AutoGrad.ipynb","provenance":[{"file_id":"1wq0Jqj2fVFsdFkmAYV0N5pzzzUdC7Z3q","timestamp":1616491087721}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"T0FLCGMZgfUn"},"source":["<img src='http://sn.nexteinstein.org/wp-content/uploads/sites/12/2016/07/aims_senegal.jpg' />"]},{"cell_type":"markdown","metadata":{"id":"PHw-5li5hGB4"},"source":["In this Tutorial we will cover:\n","\n","*   Backpropagation - Naive implementation\n","*   Computational Graphs\n","*   Backpropagation - Modular implementation\n","*   Pytorch AutoGrad\n","*   Some of torch.nn module\n"]},{"cell_type":"markdown","metadata":{"id":"2yVJ7rP8FEdJ"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oOhlpdDnlBB1"},"source":["Moustapha talked about Neural Networks and how they are a more powerfull class of models that let us model problems that are more complex and non-linear in nature.\n","\n","We also saw that NN in it's simplest form is just a chain of linear models (wx+b) followed by a non-linear activation function $\\sigma$().\n","\n","Moustapha also covered in class the main algorithm used to train neural networks which is exactly the same as what we used to do (mainly gradient descent) except here we utilize the chain rule to compute the gradient knowing that the loss has no diresct relation to all parameters.\n","\n","**Note: The loss is always scalar**"]},{"cell_type":"markdown","metadata":{"id":"jMKzI52qtFhV"},"source":["<img src='https://miro.medium.com/max/1276/1*F9capAHwl_rz2-Q8z511WQ.jpeg' />"]},{"cell_type":"markdown","metadata":{"id":"ZQNxpo9duuFZ"},"source":["<img src='https://images.contentstack.io/v3/assets/blt71da4c740e00faaa/blt3e9883f5dfd008f4/603039d9cb67827268e09219/saltbae_pytorch.jpg' />"]},{"cell_type":"markdown","metadata":{"id":"lWGLbiPDFF5r"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0OZpI5thlBFS"},"source":["let's take a two layer neural network for example to solve a binary classification problem:\n","\n","$ X \\in \\mathbb{R}^{N \\times D}, y \\in \\mathbb{R}^{N \\times 1},$ and $H$ is the hidden size.\n","\n","$out = \\sigma(\\sigma(X.W_1 + b_1).W_2 + b_2)$\n","\n","where\n","\n","$\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is the sigmoid activation function\n","\n","$W_1 \\in \\mathbb{R}^{D \\times H}, b_1 \\in \\mathbb{R}^{H}, W_2 \\in \\mathbb{R}^{H \\times 1}, b_2 \\in \\mathbb{R}$\n"]},{"cell_type":"markdown","metadata":{"id":"MJeR4C2wsaAe"},"source":["---\n","---\n","---\n","---\n","---"]},{"cell_type":"markdown","metadata":{"id":"LjTnE37RFpss"},"source":["Naive Implementation of BackProb\n","\n","Basically get pen and paper, sit down for 1 hour review vector calculus, and compute the gradient explicity for each term."]},{"cell_type":"code","metadata":{"id":"HJyn1PjrggxG","executionInfo":{"status":"ok","timestamp":1616491667996,"user_tz":0,"elapsed":3814,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["import torch\n","import math"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uMYArKhx0nP8"},"source":["\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"yhs4azR3sgv8","executionInfo":{"status":"ok","timestamp":1616491667998,"user_tz":0,"elapsed":3809,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["def sigmoid(x):\n","    return (1.0 / (1.0 + torch.exp(-x)))"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ixr7_NMf0o-C"},"source":["\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"R8JwF1Vpgg5s","executionInfo":{"status":"ok","timestamp":1616491897537,"user_tz":0,"elapsed":1206,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["def initParameters(X, y, H):\n","  '''\n","  X: Input data of shape (N, D). Each X[i] is a training sample.\n","  y: Vector of training labels. y[i] is the label for X[i]\n","  H: The Hidden size for a two layer NN\n","  '''\n","  D = X.shape[1] # number of neurons in input layer = D\n","  output_size = y.shape[1] # number of neurons in output layer.\n","\n","  W1 = torch.rand(D, H) * 1e-2\n","  b1 = torch.zeros((H))\n","  W2 = torch.rand(H, output_size) * 1e-2 \n","  b2 = torch.zeros((output_size))\n","\n","  params = {'W1': W1, 'W2': W2, 'b1': b1, 'b2': b2}\n","\n","  return params"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TJzd5y4S0qLQ"},"source":["\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"ZeB7fy-7zfWX","executionInfo":{"status":"ok","timestamp":1616491898793,"user_tz":0,"elapsed":720,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["def loss(y_hat, y):\n","  '''\n","  y_hat: predict y of shape (N, 1)\n","  y: ground truth labels of shape (N, 1)\n","  '''\n","  N = y.shape[0]\n","  loss = - torch.sum( y * torch.log(y_hat) + (1-y)*torch.log(1-y_hat) ) / N\n","  return loss"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fp0qN42c0rDn"},"source":["\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"vOaxt9uZgg9I","executionInfo":{"status":"ok","timestamp":1616491901139,"user_tz":0,"elapsed":1083,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["def forward_pass(X, params):\n","  '''\n","  X: Input data of shape (N, D). Each X[i] is a training sample.\n","  params: dictionary containing the parameters of a 2 layer NN model\n","  '''\n","\n","  S1 = torch.mm(X, params['W1']) + params['b1']\n","  A1 = sigmoid(S1)\n","  S2 = torch.mm(A1, params['W2']) + params['b2']\n","  out = sigmoid(S2) \n","\n","  cache = {'S1': S1, 'S2': S2, 'A1': A1, 'out': out}\n","\n","  return cache"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dK9tttSJ0sKq"},"source":["\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"uarQeB61zY94","executionInfo":{"status":"ok","timestamp":1616494475469,"user_tz":0,"elapsed":1022,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["def backward_pass(X, y, params, cache):\n","  '''\n","  X: Input data of shape (N, D). Each X[i] is a training sample.\n","  y: Vector of training labels. y[i] is the label for X[i]\n","  params: dictionary containing the parameters of a 2 layer NN model\n","  cache: dictionary containing the intermediate outputs of the forward pass needed to compute the grad \n","  '''\n","\n","  # Rule of thumb: Always follow the shapes\n","  # grad of W should have the same shape as W\n","\n","  N = X.shape[0]\n","\n","  grad = {}\n","\n","  dS2 = cache['out'] - y  # prove it ? I can send some links to help (N, 1)\n","  grad['W2'] = (1/N) * torch.mm(cache['A1'].t(), dS2)  # (H, 1)\n","  grad['b2'] = (1/N) * torch.sum(dS2, dim = 0, keepdims=False) # (1)\n","\n","  dS1 = torch.mm(dS2, params['W2'].t()) * sigmoid(cache['A1']) * (1 - sigmoid(cache['A1'])) # (N, H)\n","\n","  grad['W1'] = (1/N) * torch.mm(X.t(), dS1)\n","  grad['b1'] = (1/N) * torch.sum(dS1, dim= 0, keepdims=False)\n","\n","  return grad\n"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TjAHoyeK-2nj"},"source":["\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"IEHrCMzz-3be","executionInfo":{"status":"ok","timestamp":1616494455340,"user_tz":0,"elapsed":659,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["def update(params, grad, lr):\n","  params['W1'] -= lr * grad['W1']\n","  params['b1'] -= lr * grad['b1']\n","  params['W2'] -= lr * grad['W2']\n","  params['b2'] -= lr * grad['b2']\n","\n","  return params "],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aW-jZVQu-Uq3"},"source":["\n","\n","---\n","\n","let's train the model"]},{"cell_type":"code","metadata":{"id":"P3BgYGrswzcg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616494532782,"user_tz":0,"elapsed":909,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}},"outputId":"cbc7cc6b-eccb-4b8b-fdee-4bf787276773"},"source":["X = torch.rand((1000, 16)) # dummy input\n","y = 1.0 * (torch.rand((1000, 1)) > 0.5) # random output\n","\n","def fit(X, y, H=64, lr = 0.1, n_epochs=20):\n","\n","  params = initParameters(X, y, H)\n","\n","  for epoch in range(n_epochs):\n","\n","    cache = forward_pass(X, params)\n","\n","    epoch_loss = loss(cache['out'], y)\n","\n","    print('epoch ==> ', epoch, ' loss ==> ', epoch_loss.item())\n","\n","    grad = backward_pass(X, y, params, cache)\n","\n","    params = update(params, grad, lr)\n","\n","\n","fit(X, y)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["epoch ==>  0  loss ==>  0.6961341500282288\n","epoch ==>  1  loss ==>  0.6940761804580688\n","epoch ==>  2  loss ==>  0.693431556224823\n","epoch ==>  3  loss ==>  0.6932302117347717\n","epoch ==>  4  loss ==>  0.6931674480438232\n","epoch ==>  5  loss ==>  0.6931479573249817\n","epoch ==>  6  loss ==>  0.6931418180465698\n","epoch ==>  7  loss ==>  0.693139910697937\n","epoch ==>  8  loss ==>  0.6931393146514893\n","epoch ==>  9  loss ==>  0.6931390762329102\n","epoch ==>  10  loss ==>  0.6931390762329102\n","epoch ==>  11  loss ==>  0.6931390166282654\n","epoch ==>  12  loss ==>  0.6931389570236206\n","epoch ==>  13  loss ==>  0.6931390166282654\n","epoch ==>  14  loss ==>  0.6931390166282654\n","epoch ==>  15  loss ==>  0.6931389570236206\n","epoch ==>  16  loss ==>  0.6931390166282654\n","epoch ==>  17  loss ==>  0.6931390166282654\n","epoch ==>  18  loss ==>  0.6931390762329102\n","epoch ==>  19  loss ==>  0.6931390166282654\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Bs0l8OwSHCAi"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"E3ZynXOwFH7V"},"source":["As you can see this way of implementing Backprob has a lot of problems:\n","\n","*  it's not scalable, what if we want to add more layers or change the loss function or change the activatoin function ==> we will have to do it all over again.\n","\n","*  it's very tedius and prone to error.\n","\n","* it's not feasible for complex models."]},{"cell_type":"markdown","metadata":{"id":"Rs50VBcaHAGC"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"d3lPEEsFHBSE"},"source":["# Computaional Graph\n","is a directed graph that represents the computations we are performing inside our model.\n","\n","We can also see computational graphs as a way or a framework to ease the computations of gradient for us.\n","\n","Pytorch, Tensorflow, Theano ...etc, All these libraries are based on the fundamental idea of computaional graphs.\n","\n","follow on board and see slides: 56 --->  111\n","\n","We will utilize this idea of local node computational graph to improve (make modular) implementation of our Naive Backprob.\n","\n","Basically for each operation/layer we will implement a `forward` and a `backward` function. The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the `backward` pass, like this:\n","\n","```python\n","def forward(x, w):\n","  \"\"\" Receive inputs x and weights w \"\"\"\n","  # Do some computations ...\n","  z = # ... some intermediate value\n","  # Do some more computations ...\n","  out = # the output\n","   \n","  cache = (x, w, z, out) # Values we need to compute gradients\n","   \n","  return out, cache\n","```\n","\n","The `backward` pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n","\n","```python\n","def backward(dout, cache):\n","  \"\"\"\n","  Receive dout (derivative of loss with respect to outputs) and cache,\n","  and compute derivative with respect to inputs.\n","  \"\"\"\n","  # Unpack cache values\n","  x, w, z, out = cache\n","  \n","  # Use values in cache to compute derivatives\n","  dx = # Derivative of loss with respect to x\n","  dw = # Derivative of loss with respect to w\n","  \n","  return dx, dw\n","```\n","\n","After implementing a bunch of layers this way, we will be able to easily combine them to build models with different architectures.\n","\n","For each layer we implement, we will define a class with two static methods `forward` and `backward`."]},{"cell_type":"markdown","metadata":{"id":"u4-lxx6imjlV"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"naNEOYOymk80"},"source":["### Linear layer\n"]},{"cell_type":"code","metadata":{"id":"bQe9JvdxHRSM"},"source":["class Linear(object):\n","\n","  @staticmethod\n","  def forward(x, w, b):\n","    \"\"\"\n","      Inputs:\n","    - x: A tensor containing input data, of shape (N, D)\n","    - w: A tensor of weights, of shape (D, M)\n","    - b: A tensor of biases, of shape (M,)\n","    Returns a tuple of:\n","    - out: output, of shape (N, M)\n","    - cache: (x, w, b)\n","    \"\"\"\n","    N = X.shape[0]\n","    out = torch.mm(X,w) + b\n","    cache = (x, w, b)\n","    return out, cache\n","\n","  @staticmethod\n","  def backward(dout, cache):\n","      \"\"\"\n","      Computes the backward pass for an linear layer.\n","      Inputs:\n","      - dout: Upstream derivative, of shape (N, M)\n","      - cache: Tuple of:\n","        - x: Input data, of shape (N, D)\n","        - w: Weights, of shape (D, M)\n","        - b: Biases, of shape (M,)\n","      Returns a tuple of:\n","      - dx: Gradient with respect to x, of shape (N, D)\n","      - dw: Gradient with respect to w, of shape (D, M)\n","      - db: Gradient with respect to b, of shape (M,)\n","      \"\"\"\n","      x, w, b = cache\n","      N = x.shape[0]\n","\n","      db = torch.sum(dout, dim = 0, keepdims=False)\n","      dw = torch.mm(x.t(), dout)\n","      dx = torch.mm(dout, w.t())\n","      \n","      return dx, dw, db"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FUJ3RyYioJpW"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"uqVWCTSQoMHJ"},"source":["### ReLU activation"]},{"cell_type":"code","metadata":{"id":"rCeM2uTXoO0j"},"source":["class ReLU(object):\n","\n","  @staticmethod\n","  def forward(x):\n","      \"\"\"\n","      Computes the forward pass for a layer of rectified linear units (ReLUs).\n","      Input:\n","      - x: Input; a tensor of any shape\n","      Returns a tuple of:\n","      - out: Output, a tensor of the same shape as x\n","      - cache: x\n","      \"\"\"\n","      out = x * torch.gt(x, 0)\n","      cache = x\n","      return out, cache\n","\n","  @staticmethod\n","  def backward(dout, cache):\n","    \"\"\"\n","    Computes the backward pass for a layer of rectified linear units (ReLUs).\n","    Input:\n","    - dout: Upstream derivatives, of any shape\n","    - cache: Input x, of same shape as dout\n","    Returns:\n","    - dx: Gradient with respect to x\n","    \"\"\"\n","    x = cache\n","    dx = torch.gt(x, 0) * dout \n","    return dx"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"feuFitPMt_5n","executionInfo":{"status":"ok","timestamp":1616497032057,"user_tz":0,"elapsed":814,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["torch.gt?"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gSwoRywPo589"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OdEOgJL0o-aa"},"source":["### Sigmoid activation"]},{"cell_type":"code","metadata":{"id":"pr354dIAo7Em"},"source":["class sigmoid(object):\n","\n","  @staticmethod\n","  def forward(x):\n","      \"\"\"\n","      Computes the forward pass for a layer of a sigmoid input.\n","      Input:\n","      - x: Input; a tensor of any shape\n","      Returns a tuple of:\n","      - out: Output, a tensor of the same shape as x\n","      - cache: x\n","      \"\"\"\n","      out = 1.0 / (1.0 + torch.exp(-x))\n","      cache = out\n","      return out, cache\n","\n","  @staticmethod\n","  def backward(dout, cache):\n","    \"\"\"\n","    Computes the backward pass for a layer of rectified linear units (ReLUs).\n","    Input:\n","    - dout: Upstream derivatives, of any shape\n","    - cache: Input output of the sigmoid, of same shape as dout\n","    Returns:\n","    - dx: Gradient with respect to x\n","    \"\"\"\n","    out = cache\n","    dx = out * (1-out) * dout \n","    return dx"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8vscAsTvuIaT"},"source":["\n","\n","---\n","\n","now we have all the building blocks, we can train a 2 layer network using this strategy:\n","\n"]},{"cell_type":"code","metadata":{"id":"g4GCwPmLuUWh"},"source":["class TwoLayerNet(object):\n","\n","  def __init__(self, input_dim, hidden_dim=100, output_dim = 1, weight_scale=1e-3):\n","    \"\"\"\n","    Initialize a new network.\n","    Inputs:\n","    - input_dim: An integer giving the size of the input\n","    - hidden_dim: An integer giving the size of the hidden layer\n","    \"\"\"\n","    self.params = {}\n","    self.params['W1'] = weight_scale * torch.randn((input_dim, hidden_dim))\n","    self.params['b1'] = torch.zeros((hidden_dim))\n","    self.params['W2'] = weight_scale * torch.randn((hidden_dim, output_dim))\n","    self.params['b2'] = torch.zeros((output_dim))\n","\n","  def loss(self, y_hat, y):\n","\n","    N = y.shape[0]\n","    loss = - torch.sum( y * torch.log(y_hat) + (1-y)*torch.log(1-y_hat) ) / N\n","    dloss = (y_hat - y)/(y_hat * (1 - y_hat))\n","    return loss, dloss\n","\n","\n","  def one_pass(self, X, y=None):\n","    \"\"\"\n","    Compute loss and gradient for a minibatch of data.\n","\n","    Inputs:\n","    - X: Tensor of input data of shape (N, D)\n","    - y: Tensor of labels, of shape (N,). y[i] gives the label for X[i].\n","\n","    Returns:\n","    - loss: Scalar value giving the loss\n","    - grads: Dictionary with the same keys as self.params, mapping parameter\n","      names to gradients of the loss with respect to those parameters.\n","    \"\"\"\n","    W1 = self.params['W1'] \n","    b1 = self.params['b1'] \n","    W2 = self.params['W2'] \n","    b2 = self.params['b2'] \n","\n","    A1, linear_cache_1 = Linear.forward(X, W1, b1)\n","    S1, relu_cache = ReLU.forward(A1)\n","    S2, linear_cache_2 = Linear.forward(h1, W2, b2)\n","    out, sigmoidcache = sigmoid.forward(S2)\n","\n","    loss, grads = 0, {}\n","  \n","    l, dloss = loss(out, y)\n","    \n","    dS2 = sigmoid.backward(dloss, sigmoidcache)\n","    dS1, grads['W2'], grads['b2'] = Linear.backward(dS2, linear_cache_2)\n","    dA1, = ReLU.backward(dS1, relu_cache)\n","    dX, grads['W1'], grads['b1'] =  Linear.backward(dA1, linear_cache_1)\n","  \n","    return loss, grads"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D9NPEsC5qUDH"},"source":["# **But do we need all of This to train neural networks?**"]},{"cell_type":"markdown","metadata":{"id":"LDUmxWKRyg9T"},"source":["---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"XALKYBIzxDGF"},"source":["Any deep learning framework should support some of the following features:\n","\n","Fast prototyping, Automatically computing the gradient and to Accelerate computations by utillizing gpus.\n","\n","If you ever visit the github repo of pytorch, you will find the difinition of it as:\n","\n","PyTorch is a Python package that provides two high-level features:\n","\n","* Tensor computation (like NumPy) with strong GPU acceleration.\n","\n","* Deep neural networks built on a tape-based autograd system.\n","\n","Now let's explore the power of pytorch."]},{"cell_type":"markdown","metadata":{"id":"0nu-C25ozpnw"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tGqRw-PN1b5C"},"source":["Francis told you that Tensors in pytorch are just like nd-arrays in numpy, while this right, because they share many attributes like (shape, size, dtype...etc). PyTorch Tensors are more powerfull as they support some additional enhancements which make them unique: \n","\n","Apart from CPU, they can be loaded on the GPU for faster computations using the `.device` attribute. A similarlly important feature of them is that when setting `.requires_grad = True` pytorch autograd engine start forming a graph that tracks every operation applied on them to calculate the gradients using the same idea of computaional graphs we talked about it previously.\n","\n","Pytorch Autograd is an engine to calculate derivatives. It records a graph of all the operations performed on a gradient enabled tensor and creates an acyclic graph called the dynamic computational graph. The leaves of this graph are input tensors and the roots are output tensors. Gradients are calculated by tracing the graph from the root to the leaf and multiplying every gradient in the way using the chain rule.\n","\n","if you still don't get it I recommend this video: https://www.youtube.com/watch?v=MswxJw-8PvE&t=645s"]},{"cell_type":"code","metadata":{"id":"gfsqdA1l2711","executionInfo":{"status":"ok","timestamp":1616499466676,"user_tz":0,"elapsed":4301,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["import torch\n","x = torch.rand((2, 3))"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"56u0hzLF3N_X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616499596196,"user_tz":0,"elapsed":637,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}},"outputId":"21cbfa9b-af2e-4f8a-deb4-3674eb28f487"},"source":["x = x.to(device = torch.device('cuda'))\n","x.device\n","#torch.cuda.is_available()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"pjnlU2SE3KiU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616499619457,"user_tz":0,"elapsed":894,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}},"outputId":"e0ccd707-8ea9-4c6e-dd01-d1e5709b2047"},"source":["x.requires_grad"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"A4vqFAfI3QPo","executionInfo":{"status":"ok","timestamp":1616499625918,"user_tz":0,"elapsed":772,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["x.grad #Holds the gradient of x, for now it's None"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"SaFq_rVs3TbC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616499633136,"user_tz":0,"elapsed":1484,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}},"outputId":"89a27f3e-7d3f-4abd-d41e-aa278aba1823"},"source":["x.is_leaf"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"m_azoL5w3b7D","executionInfo":{"status":"ok","timestamp":1616499636825,"user_tz":0,"elapsed":1088,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["x.grad_fn #references a Function that has created the Variable used to calculate the gradient, for now it's None"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WBkcEGEy24jx"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dFeN2ndezqq-"},"source":["The autograd package provides automatic differentiation for all operations on Tensors. Once you finish your computation you can call The magic word `.backward()` and have all the gradients computed automatically."]},{"cell_type":"code","metadata":{"id":"TL-Er523zqB2","executionInfo":{"status":"ok","timestamp":1616499783412,"user_tz":0,"elapsed":818,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["a = torch.tensor(3.0, requires_grad=True)\n","b = torch.tensor(5.0, requires_grad=True)\n","\n","c = a * b\n","c.backward()"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"XgA2nsbFxif-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616499783720,"user_tz":0,"elapsed":982,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}},"outputId":"bf739506-2342-414e-9202-c56a8bb2e0ee"},"source":["a.grad"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(5.)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"6VovWA8t6LZn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616499783721,"user_tz":0,"elapsed":814,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}},"outputId":"99db96ff-ccb9-4ae8-ac9f-d29bbf37c62d"},"source":["b.grad"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(3.)"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"qE0NI-L86Nea","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616499784001,"user_tz":0,"elapsed":763,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}},"outputId":"09368123-3a6b-49a0-df3e-d9aad8a307cb"},"source":["c.grad_fn"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<MulBackward0 at 0x7fa8235b8f90>"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"mqS3B4Lv78Nv"},"source":["<img src='https://miro.medium.com/max/589/1*viCEZbSODfA8ZA4ECPwHxQ.png' />"]},{"cell_type":"markdown","metadata":{"id":"iQL8s1g16vmJ"},"source":["---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"O1JvZ6PS64V5"},"source":["Know we know autograd let's use it to train neural networks ===> first we will watch justin explaining it lecture 9 (34:30 -> 51:00) :)\n","\n","---\n","---\n","---\n","\n","In the next few cells I will give a simple example of using torch.nn module, But it's not enough to cover all of it so please read this: https://pytorch.org/tutorials/beginner/nn_tutorial.html when you go back \n","\n","we will use Mini batch SGD to solve a regression toy example using a custom multilayer neural network utillizing torch.nn module. "]},{"cell_type":"code","metadata":{"id":"L-tI24VY8Lvb","executionInfo":{"status":"ok","timestamp":1616501939652,"user_tz":0,"elapsed":829,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["import torch\n","import torch.nn as nn\n","from torch import optim"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mv2dhJw6CTIP","executionInfo":{"status":"ok","timestamp":1616501942703,"user_tz":0,"elapsed":958,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["if torch.cuda.is_available():\n","  device = torch.device('cuda')\n","else:\n","  device = torch.device('cpu')"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dscq8P0WBHdi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616502208571,"user_tz":0,"elapsed":1108,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}},"outputId":"0eb1da0a-4484-49d7-c8a6-1d790c0d123b"},"source":["N, D_in, H1, H2, H3, D_out =  1280, 16, 32, 64, 32, 1\n","\n","X = torch.rand((N, D_in), device=device)\n","y = torch.rand((N, D_out), device=device)\n","\n","model = nn.Sequential(\n","    nn.Linear(D_in, H1),\n","    nn.ReLU(),\n","    nn.Linear(H1, H2),\n","    nn.ReLU(),\n","    nn.Linear(H2, H3),\n","    nn.ReLU(),\n","    nn.Linear(H3, D_out)\n",")\n","\n","model = model.to(device = device) # moving the model to the same device\n","\n","optimizer = optim.SGD(model.parameters(), lr=0.01) # or optimizer = optim.SGD((model.parameters(), lr=0.01, momentum=0.9)\n","criterion = nn.MSELoss()\n","\n","epochs = 10 \n","bs = 32\n","n_batch = int(N/bs)\n","\n","for epoch in range(epochs):\n","    epoch_loss = 0\n","    for i in range(n_batch):\n","        \n","        start_i = i * bs\n","        end_i = start_i + bs\n","\n","        xb = X[start_i:end_i]\n","        yb = y[start_i:end_i]\n","\n","        y_pred = model(xb)\n","\n","        loss = criterion(y_pred, yb)\n","        epoch_loss += loss.item()\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    epoch_loss /= N\n","    print('epoch ==> ', epoch, 'epoch loss ==> ', epoch_loss)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["epoch ==>  0 epoch loss ==>  0.0050031937134917825\n","epoch ==>  1 epoch loss ==>  0.002691714378306642\n","epoch ==>  2 epoch loss ==>  0.0026300780358724297\n","epoch ==>  3 epoch loss ==>  0.0026272242597769947\n","epoch ==>  4 epoch loss ==>  0.002626016165595502\n","epoch ==>  5 epoch loss ==>  0.002624938345979899\n","epoch ==>  6 epoch loss ==>  0.0026239793456625192\n","epoch ==>  7 epoch loss ==>  0.002623062179191038\n","epoch ==>  8 epoch loss ==>  0.0026221613807138056\n","epoch ==>  9 epoch loss ==>  0.0026212684984784572\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Y6SEmsAPGdr5"},"source":["Another way (more common) to create a model in pytorch using the nn module is to extend the nn.module class"]},{"cell_type":"code","metadata":{"id":"44rTsrQwGsQF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616502953876,"user_tz":0,"elapsed":764,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}},"outputId":"98122fd6-0981-4e24-de3a-4cf087842bf5"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# In the class we want to extend the module: nn.Module\n","class Net(nn.Module):\n","\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # 1 input image channel, 6 output channels, 3x3 square convolution\n","        # kernel\n","        self.conv1 = nn.Conv2d(1, 6, 3)\n","        self.conv2 = nn.Conv2d(6, 16, 3)\n","        # an affine operation: y = Wx + b\n","        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        # Max pooling over a (2, 2) window\n","        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n","        # If the size is a square you can only specify a single number\n","        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n","        x = x.view(-1, self.num_flat_features(x))\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","        \n","net = Net()\n","print(net)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Net(\n","  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n","  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n","  (fc1): Linear(in_features=576, out_features=120, bias=True)\n","  (fc2): Linear(in_features=120, out_features=84, bias=True)\n","  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KS1C7EB8HESr"},"source":["Now There are ton of things to cover in pytorch: \n","\n","DataLoaders: explicit Minibatch implementation with options to do data augmentation.\n","\n","Different loss functions: cross entropy (for multiclass classification), Nigative log liklihood (for binary classification), CTC loss (mostly used for speech data) ... etc\n","\n","other layers: 2dconv, 3dconv, rnn, lstm, gru, attention ... etc.\n","\n","other optimizers: Adam, RMSProp, Adagrad ... etc\n","\n","Regularizers: l2 (weight decay), dropout, BatchNorm ... etc\n","\n","how to save models, torch.save()\n","\n","learning rate schedulers: cosine, step, ... etc\n","\n","activation functions: relu, sigmoid, tanh, swish, leaky relu, elu, ...etc\n","\n","pretrained models: ResNets, Vgg, BERT, ... etc\n","\n","\n","\n","and alot alot alot more: We will try to cover as much as possible But you have to work with us: Best Thing to do is watch and do the assignments in this course:\n","https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/\n","\n","Also check the solutions for the deep learning nano degree from udacity: https://github.com/udacity/deep-learning-v2-pytorch"]},{"cell_type":"markdown","metadata":{"id":"A65cYkRAIsjz"},"source":["# Thank you, Assignment soon to be released"]},{"cell_type":"code","metadata":{"id":"4DlG_oWvKtLx"},"source":[""],"execution_count":null,"outputs":[]}]}