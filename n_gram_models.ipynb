{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2+"},"colab":{"name":"Copy of n_gram_models.ipynb","provenance":[{"file_id":"1UU1cRm0gXHPNKwrI35yJPR0LtS7cCzjm","timestamp":1619773181067}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"UIxPUlQW_790"},"source":["\n","<h1 style=\"font-family:verdana;font-size:300%;text-align:center;background-color:#f2f2f2;color:#0d0d0d\">AMMI NLP - Review sessions</h1>\n","\n","<h1 style=\"font-family:verdana;font-size:180%;text-align:Center;color:#993333\"> Lab 3: n-gram models </h1>"]},{"cell_type":"markdown","metadata":{"id":"oFb3QaVv_798"},"source":["**Big thanks to Amr Khalifa who improved this lab and made it to a Jupyter Notebook!**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4N3NcoPgAYcG","executionInfo":{"status":"ok","timestamp":1619773235874,"user_tz":0,"elapsed":30134,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}},"outputId":"58bd97a6-649b-4ba1-967b-b6a52a2a378f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Yb-VL0Xt_798","executionInfo":{"status":"ok","timestamp":1619774626743,"user_tz":0,"elapsed":1151,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["import io, sys, math, re\n","from collections import defaultdict\n","import numpy as np"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"fezRMhzM_799","executionInfo":{"status":"ok","timestamp":1619774626744,"user_tz":0,"elapsed":812,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}}},"source":["# data_loader\n","def load_data(filename):\n","    '''\n","    parameters:\n","    filename (string): datafile\n","    \n","    Returns:\n","    data (list of lists): each list is a sentence of the text \n","    vocab (dictionary): {word: no of times it appears in the text}\n","    '''\n","    fin = io.open(filename, 'r', encoding='utf-8')\n","    data = []\n","    vocab = defaultdict(lambda:0)\n","    for line in fin:\n","        sentence = line.split()\n","        data.append(sentence)\n","        for word in sentence:\n","            vocab[word] += 1\n","    return data, vocab"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"KKFkQWNt_799","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619774630075,"user_tz":0,"elapsed":2753,"user":{"displayName":"Engelbert Tchinde Wamba","photoUrl":"","userId":"14616578628350736220"}},"outputId":"dbd79f17-67f0-4028-b2c2-20305ec3e371"},"source":["\n","print(\"load training set..\")\n","print(\"\\n\")\n","train_data, vocab = load_data(\"/content/drive/MyDrive/session3_n-gram_NLP/train1.txt\")\n","print(train_data[0])\n","print(\"\\n\")\n","print(\"how :\",vocab['how'])\n","print(\"load validation set\")\n","valid_data, _ = load_data(\"/content/drive/MyDrive/session3_n-gram_NLP/valid1.txt\")\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["load training set..\n","\n","\n","['<s>', 'my', 'fathers', \"don't\", 'speak', 'dutch.', '</s>']\n","\n","\n","how : 107\n","load validation set\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kiLz3f6o_79-"},"source":["def remove_rare_words(data, vocab, mincount = 1):\n","    '''\n","    Parameters:\n","    data (list of lists): each list is a sentence of the text \n","    vocab (dictionary): {word: no of times it appears in the text}\n","    mincount(int): the minimum count \n","    \n","    Returns: \n","    data_with_unk(list of lists): data after replacing rare words with <unk> token\n","    '''\n","    # replace words in data that are not in the vocab \n","    # or have a count that is below mincount\n","    data_with_unk = []\n","\n","    ## FILL CODE\n","    \n","    return data_with_unk"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7CUVLfzl_79-"},"source":["print(\"remove rare words\")\n","train_data = remove_rare_words(train_data, vocab, mincount = 1)\n","valid_data = remove_rare_words(valid_data, vocab, mincount = 1)\n","#train_data\n","print(train_data[0])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"te2rLU49_79-"},"source":["def build_ngram(data, n):\n","    '''\n","    Parameters:\n","    data (list of lists): each list is a sentence of the text \n","    n (int): size of the n-gram\n","    \n","    Returns:\n","    prob (dictionary of dictionary)\n","    {\n","        context: {word:probability of this word given context}\n","    }\n","    '''\n","    total_number_words = 0\n","    counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n","\n","    for sentence in data:\n","        sentence = tuple(sentence)\n","        ## FILL CODE\n","        # dict can be indexed by tuples\n","        # store in the same dict all the ngrams\n","        # by using the context as a key and the word as a value\n","                           \n","\n","    prob = defaultdict(lambda: defaultdict(lambda: 0.0))\n","    # Build the probabilities from the counts\n","    # Be careful with how you normalize!\n","\n","    for context in counts.keys():\n","    ## FILL CODE\n","\n","    return prob"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xdpn2gfV_79_"},"source":["# RUN TO BUILD NGRAM MODEL\n","\n","n = 2\n","print(\"build ngram model with n = \", n)\n","model = build_ngram(train_data, n)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JRY70RvQ_79_"},"source":["Here, implement a recursive function over shorter and shorter context to compute a \"stupid backoff model\". An interpolation model can also be implemented this way."]},{"cell_type":"code","metadata":{"id":"tVswkCcv_79_"},"source":["def get_prob(model, context, w):\n","    '''\n","    Parameters: \n","    model (dictionary of dictionary)\n","    {\n","        context: {word:probability of this word given context}\n","    } \n","    context (list of strings): a sentence\n","    w(string): the word we need to find it's probability given the context\n","    \n","    Retunrs:\n","    prob(float): probability of this word given the context \n","    '''\n","\n","    # code a recursive function over \n","    # smaller and smaller context\n","    # to compute the backoff model\n","    \n","    ## FILL CODE\n","\n","    return 0.0\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nc0iq8D0_79_"},"source":["def perplexity(model, data, n):\n","    '''\n","    Parameters: \n","    model (dictionary of dictionary)\n","    {\n","        context: {word:probability of this word given context}\n","    } \n","    data (list of lists): each list is a sentence of the text\n","    n(int): size of the n-gram\n","    \n","    Retunrs:\n","    perp(float): the perplexity of the model \n","    '''\n","    ## FILL CODE\n","\n","    return 0.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZyeKTrQs_7-A"},"source":["# COMPUTE PERPLEXITY ON VALIDATION SET\n","\n","print(\"The perplexity is\", perplexity(model, valid_data, n=n))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"83n5nBJt_7-A"},"source":["def get_proba_distrib(model, context):\n","    ## need to get the the words after the context and their probability of appearance\n","    ## after this context \n","    '''\n","    Parameters: \n","    model (dictionary of dictionary)\n","    {\n","        context: {word:probability of this word given context}\n","    }\n","    context (list of strings): the sentence we need to find the words after it and \n","    thier probabilites\n","    \n","    Retunrs:\n","    words_and_probs(dic): {word: probability of word given context}\n","    \n","    '''\n","    # code a recursive function over context\n","    # to find the longest available ngram\n","    \n","    ## FILL CODE\n","    \n","    return None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e8RmBNIp_7-A"},"source":["def generate(model):\n","    '''\n","    Parameters: \n","    model (dictionary of dictionary)\n","    {\n","        context: {word:probability of this word given context}\n","    }\n","    \n","    Retunrs:\n","    sentence (list of strings): a sentence sampled according to the language model. \n","    \n","\n","    '''\n","    # generate a sentence. A sentence starts with a <s> and ends with a </s>\n","    # Possiblly a use function is:\n","    # np.random.choice(x, 1, p = y)\n","\n","    # where x is a list of things to sample from\n","    # and y is a list of probability (of the same length as x)\n","    sentence = [\"<s>\"]\n","    while sentence[-1] != \"</s>\" and len(sentence)<100:\n","        ## FILL CODE\n","        \n","    return sentence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bu62MpMj_7-A"},"source":["# GENERATE A SENTENCE FROM THE MODEL\n","\n","print(\"Generated sentence: \",generate(model))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q8VZpZbn_7-B"},"source":["Once you are done implementing the model, evaluation and generation code, you can try changing the value of `n`, and play with a larger training set (`train2.txt` and `valid2.txt`). You can also try to implement an interpolation model."]},{"cell_type":"code","metadata":{"id":"bMG2Kr_C_7-B"},"source":[""],"execution_count":null,"outputs":[]}]}